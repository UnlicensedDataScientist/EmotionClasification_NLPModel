{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2b1d03",
   "metadata": {},
   "source": [
    "# classification of emotions\n",
    "\n",
    "This is a multi-label classification problem in which you have to label a set \n",
    "of tweets according to 11 feelings ('anger', 'anticipation', 'disgust', 'fear', 'joy', 'love', \n",
    "'optimism', 'pessimism', 'sadness', 'surprise', 'trust')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cdffa9",
   "metadata": {},
   "source": [
    "## Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bd4cff-b045-4504-8938-e935d7cae76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, AutoConfig, TFAutoModelForSequenceClassification, BertTokenizer, BertForSequenceClassification\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from transformers import TFAutoModel\n",
    "import transformers\n",
    "import keras\n",
    "import re, string, spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from configLogger import *\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df005ece",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d72cbfa-94ca-4d9a-baa5-69d81d69b622",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sem_eval_train_es.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a06d68a-1610-4524-bcf3-832f7729547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape, df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df425e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847143e4-4d11-4746-902c-b778aaf1ef7e",
   "metadata": {},
   "source": [
    "## Data Processing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8661426-890f-4212-b676-b849368f1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingCleanData():\n",
    "    def __init__(self, df:pd.DataFrame, clean_emoji:bool, typ_data:str='train'):\n",
    "        self.df = df\n",
    "        self.clean_emoji = clean_emoji\n",
    "        self.typ_data = typ_data\n",
    "        \n",
    "        if self.typ_data == 'test':\n",
    "            pass\n",
    "        else:\n",
    "            self.sentiment_columns = self.df.columns[2:]\n",
    "            \n",
    "        self.id11label = {}\n",
    "        self.label11id = {}\n",
    "        \n",
    "    def clean_text(self, text:str) -> str:\n",
    "        pattern1 = re.compile(r'@[\\w_]+') #elimina menciones\n",
    "        pattern2 = re.compile(r'https?://[\\w_./]+') #elimina URL\n",
    "        pattern4 = re.compile('[{}]+'.format(re.escape(string.punctuation))) #elimina símbolos de puntuación\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                   \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "        \"\"\"Limpiamos las menciones, URL y hashtags del texto. Luego \n",
    "        quitamos signos de puntuación\"\"\"\n",
    "        text = pattern1.sub('', text)\n",
    "        text = pattern2.sub('', text)\n",
    "        text = pattern4.sub('', text)\n",
    "        if self.clean_emoji:        \n",
    "            text = emoji_pattern.sub('', text)\n",
    "\n",
    "        return text\n",
    "    def verify_row_without_clasification(self):\n",
    "        logger.warning(f\"\"\" En el conjunto de datos hay un total de: {self.df[\n",
    "            (self.df.anger == False) \n",
    "           & (self.df.anticipation == False)\n",
    "           & (self.df.disgust == False) \n",
    "           & (self.df.fear == False) \n",
    "           & (self.df.joy == False) \n",
    "           & (self.df.joy == False) \n",
    "           & (self.df.love == False)\n",
    "          & (self.df.optimism == False)\n",
    "          & (self.df.pessimism == False)\n",
    "          & (self.df.sadness == False)\n",
    "          & (self.df.surprise == False)\n",
    "          & (self.df.trust == False)\n",
    "          ].shape[0]} mensajes que no tienen ninguna clasificación\"\"\")\n",
    "        \n",
    "        self.df = self.df[~(\n",
    "                (self.df.anger == False) \n",
    "               & (self.df.anticipation == False)\n",
    "               & (self.df.disgust == False) \n",
    "               & (self.df.fear == False) \n",
    "               & (self.df.joy == False) \n",
    "               & (self.df.joy == False) \n",
    "               & (self.df.love == False)\n",
    "              & (self.df.optimism == False)\n",
    "              & (self.df.pessimism == False)\n",
    "              & (self.df.sadness == False)\n",
    "              & (self.df.surprise == False)\n",
    "              & (self.df.trust == False)\n",
    "            )].reset_index(drop=True)\n",
    "        \n",
    "        logger.info(f'La longitud del dataframe con mensajes que contienen al menos una clasificación es: {len(self.df)}')\n",
    "        \n",
    "    def replace_true_false(self):\n",
    "        self.df = self.df.replace({True:1, False:0})\n",
    "        \n",
    "    def create_dict_class(self):\n",
    "        for i, col in enumerate(self.sentiment_columns):\n",
    "            self.id11label[i] = col\n",
    "\n",
    "        self.label11id = {val: key for key, val in self.id11label.items()}\n",
    "        \n",
    "    def run_all(self):\n",
    "        logger.info(f'Limpieza de mensajes')\n",
    "        self.df['text_clean'] = self.df.Tweet.map(lambda x: self.clean_text(x))\n",
    "        \n",
    "        logger.info(f'Eliminar mensajes que no tienen ninguna clasificación.')\n",
    "        self.verify_row_without_clasification()\n",
    "        \n",
    "        \n",
    "        logger.info(f'Creando id11label y label11id')\n",
    "        self.create_dict_class()\n",
    "\n",
    "        logger.info('Reemplazando True y False por ceros y unos')\n",
    "        self.replace_true_false()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18130e7-2fe6-4bc4-898a-493304f05f8a",
   "metadata": {},
   "source": [
    "### Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffd6873-1b48-452a-b3ca-f413b5214bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_clean = ProcessingCleanData(df=df, clean_emoji=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fedac-963e-4ba3-9a33-a21ff1421b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_clean.run_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062bd7f6-cdd3-4827-9dce-547df51ed60b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_clean.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac785179-ad18-4d91-a3be-6988c9348a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_clean.id11label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cd36d9-29e1-4af8-845d-6d754e2869cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "059dc65c-a5a7-4113-891c-b1ad0b911fc1",
   "metadata": {},
   "source": [
    "### Preparing from the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784cfecc-7543-477d-9125-302fd8aba6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessingDataForModel():\n",
    "    \n",
    "    def __init__(self, df:pd.DataFrame, text_column:str, sentiment_columns:list):\n",
    "        self.df = df\n",
    "        self.text_column = text_column\n",
    "        self.sentiment_columns = sentiment_columns\n",
    "        self.labels = []\n",
    "        self.X_train_tweets = np.array([])\n",
    "        self.X_test_tweets = np.array([])\n",
    "        self.Y_train = np.array([])\n",
    "        self.Y_test = np.array([])\n",
    "        self.MAX_SEQUENCE_LENGTH = np.nan\n",
    "        \n",
    "    def create_labels(self):\n",
    "        \n",
    "        for _, row in self.df.iterrows():\n",
    "            label = [int(row[column]) for column in self.sentiment_columns]\n",
    "            self.labels.append(label)\n",
    "\n",
    "        self.labels = [[label for label in label_list] for label_list in self.labels]\n",
    "        self.labels = np.array(self.labels)\n",
    "        \n",
    "        \n",
    "    def split_data_train_test(self):\n",
    "        self.X_train_tweets, self.X_test_tweets, self.Y_train, self.Y_test = train_test_split(\n",
    "                                                                                        self.df[self.text_column],\n",
    "                                                                                        self.labels,\n",
    "                                                                                        test_size = 0.3,\n",
    "                                                                                        random_state = 0\n",
    "                                                                                        )\n",
    "\n",
    "        logger.info(f'Shape X_train: {self.X_train_tweets.shape}, Shape Y_train: {self.Y_train.shape}')\n",
    "        logger.info(f'Shape X_test: {self.X_test_tweets.shape}, Shape Y_test: {self.Y_test.shape}')\n",
    "    \n",
    "\n",
    "        \n",
    "    def max_lenght_document(self):\n",
    "\n",
    "        self.MAX_SEQUENCE_LENGTH = np.max([len(l.split()) for l in self.X_train_tweets])\n",
    "        logger.info(f'longitud máxima: {self.MAX_SEQUENCE_LENGTH}')\n",
    "        \n",
    "    def run_all(self):\n",
    "        logger.info(f'Creando variable labels')\n",
    "        self.create_labels()\n",
    "        \n",
    "        logger.info('Dividiendo el conjunto de datos en entrenamiento y prueba')\n",
    "        self.split_data_train_test()\n",
    "        \n",
    "        logger.info('Creando la variable de longitud máxima del documento')\n",
    "        self.max_lenght_document()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02010ae-d378-4a56-b874-d38ab3f01f0b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_processing_data = ProcessingDataForModel(\n",
    "                            df                = obj_clean.df, \n",
    "                            text_column       = 'text_clean', \n",
    "                            sentiment_columns = obj_clean.sentiment_columns\n",
    "                            )\n",
    "obj_processing_data.run_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459396f5-5057-40a0-921c-c98bf41775fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model create and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43225ad6-0641-4bc8-a4c5-5171e662fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreateModel():\n",
    "    def __init__(self, X_train_tweets: pd.Series, X_test_tweets: pd.Series, Y_train: pd.Series, Y_test: pd.Series, model_name: str, id11label: dict, label11id: dict, function_activation: str, n_epochs: int, batch_size: int):\n",
    "        self.X_train_tweets = X_train_tweets\n",
    "        self.X_test_tweets = X_test_tweets\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "        self.model_name = model_name\n",
    "        self.id11label = id11label\n",
    "        self.label11id = label11id\n",
    "        self.function_activation = function_activation\n",
    "        self.n_epochs = n_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.MAX_SEQUENCE_LENGTH = 0\n",
    "        self.train_encodings = transformers.tokenization_utils_base.BatchEncoding()\n",
    "        self.test_encodings = transformers.tokenization_utils_base.BatchEncoding()\n",
    "        self.train_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.model = None\n",
    "        self.history = None\n",
    "\n",
    "    def tokenize_encode_data(self):\n",
    "        # We tokenize and encode as a Dataset\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.train_encodings = tokenizer(self.X_train_tweets.to_list(), truncation=True, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "        self.MAX_SEQUENCE_LENGTH = self.train_encodings['input_ids'].shape[1]\n",
    "        self.test_encodings = tokenizer(self.X_test_tweets.to_list(), truncation=True, padding='max_length', max_length=self.MAX_SEQUENCE_LENGTH, return_tensors=\"tf\")\n",
    "\n",
    "    def create_train_test_dataset(self):\n",
    "        self.train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.train_encodings),\n",
    "            self.Y_train\n",
    "        ))\n",
    "        \n",
    "        self.test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(self.test_encodings),\n",
    "            self.Y_test\n",
    "        ))\n",
    "\n",
    "    def create_model(self):\n",
    "        # Load transformers config and set output_hidden_states to False\n",
    "        config = AutoConfig.from_pretrained(self.model_name, hidden_dropout_prob=0.1, num_labels=11, id2label=self.id11label, label2id=self.label11id)\n",
    "\n",
    "        # Load the Transformers BERT model\n",
    "        transformer_model = TFAutoModelForSequenceClassification.from_pretrained(self.model_name, config=config)\n",
    "        transformer_model.config.num_labels = 11\n",
    "        self.model = transformer_model\n",
    "\n",
    "    def train_model(self):\n",
    "        # Define the checkpoint to save the best model\n",
    "        checkpoint = ModelCheckpoint('best_model_sentiment.tf', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "        # Define early stopping to stop training after 5 epochs without improvement\n",
    "        early_stopping = EarlyStopping(monitor='val_accuracy', patience=5, mode='max')\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "        self.model.classifier = tf.keras.layers.Dense(11, activation=self.function_activation)\n",
    "        loss = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "        metric = tf.keras.metrics.BinaryAccuracy('accuracy')\n",
    "        self.model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "        self.history = self.model.fit(self.train_dataset.batch(self.batch_size), \n",
    "                                      epochs=self.n_epochs, \n",
    "                                      batch_size=self.batch_size, \n",
    "                                      validation_data=self.test_dataset.batch(self.batch_size),\n",
    "                                      callbacks=[checkpoint, early_stopping])  # Agregar los callbacks aquí\n",
    "        \n",
    "    def predict(self, new_messages: list[str]):\n",
    "        # We tokenize and encrypt new messages\n",
    "        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        new_encodings = tokenizer(new_messages, truncation=True, padding='max_length', max_length=self.MAX_SEQUENCE_LENGTH, return_tensors=\"tf\")\n",
    "\n",
    "        # We convert the new encoded messages into a Dataset\n",
    "        new_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "            dict(new_encodings)\n",
    "        ))\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = self.model.predict(new_dataset.batch(self.batch_size))\n",
    "\n",
    "        # To convert predictions into labels\n",
    "        predicted_labels = [self.id11label[np.argmax(prediction)] for prediction in predictions]\n",
    "        \n",
    "        return predicted_labels\n",
    "\n",
    "    def run_all(self, train_model=True):\n",
    "        self.tokenize_encode_data()\n",
    "        self.create_train_test_dataset()\n",
    "        self.create_model()\n",
    "        \n",
    "        if train_model:\n",
    "            self.train_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dabea0-ecb8-4f97-86d1-33d34020782e",
   "metadata": {},
   "source": [
    "### Run model bert-base-multilingual-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2d9cbd-0315-497b-9bdd-d781e3cec4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_model = CreateModel(\n",
    "            X_train_tweets      = obj_processing_data.X_train_tweets,  \n",
    "            X_test_tweets       = obj_processing_data.X_test_tweets,\n",
    "            Y_train             = obj_processing_data.Y_train,\n",
    "            Y_test              = obj_processing_data.Y_test, \n",
    "            model_name          = 'bert-base-multilingual-uncased',\n",
    "            id11label           = obj_clean.id11label,\n",
    "            label11id           = obj_clean.label11id,\n",
    "            function_activation = 'softmax',\n",
    "            n_epochs            = 30,\n",
    "            batch_size          = 25\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c0103-faf1-436e-88da-88ee82f2d66b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_model.run_all(train_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554da126-b35f-4f70-9de2-73a5124e3ba6",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c19ea-890d-4135-8561-17147b7d685a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('best_model_sentiment.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d321433-d4db-4781-b976-6dda91976de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict(obj_model.test_dataset.batch(obj_model.batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f34e92-53bc-4118-9ea6-d6fb8729a1b4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_indices = np.argmax(predict['logits'], axis=1)\n",
    "true_result = np.argmax(obj_model.Y_test, axis=1)\n",
    "\n",
    "# Get the name of the emotion corresponding to each index\n",
    "emotions = [obj_clean.id11label[index] for index in max_indices]\n",
    "emotions_true = [obj_clean.id11label[index] for index in true_result]\n",
    "\n",
    "# Print the emotions corresponding to each prediction\n",
    "count = 0\n",
    "for i, emotion in enumerate(emotions):\n",
    "    if emotion != emotions_true[i]:\n",
    "        count+=1\n",
    "        # logger.info(f'{emotion}, {emotions_true[i]}')\n",
    "    logger.info(f\"Predicción {i+1}: {emotion}. Valor real: {emotions_true[i]}\")\n",
    "logger.info(f\"El modelo predice como principal {count} resultados diferentes de {len(predict['logits'])}. No obstante no es representativo ya que puede haber más de un sentimiento por Tweet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea014b7e-f6d5-47db-885b-ea1dbf4a2a0d",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744912a8-689b-4620-9fa1-da14250a2cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test dataset\n",
    "df_test = pd.read_csv('sem_eval_test_grupo_01.csv')\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92151b-081f-4dab-9091-ef4ab2e54610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to process the test data set for model validation\n",
    "obj_clean_test = ProcessingCleanData(df=df_test, clean_emoji=True, typ_data='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3450874-4dd9-4590-ae54-494ec0e6d160",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj_clean_test.df['text_clean'] = obj_clean_test.df.Tweet.map(lambda x: obj_clean_test.clean_text(x))\n",
    "obj_clean_test.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45e39f-b7b2-4c07-ba53-cee47a0e39ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_(df, new_messages: list[str], model):\n",
    "    # We tokenize and encrypt new messages\n",
    "    tokenizer = AutoTokenizer.from_pretrained(obj_model.model_name)\n",
    "    new_encodings = tokenizer(new_messages, truncation=True, padding='max_length', max_length=obj_model.MAX_SEQUENCE_LENGTH, return_tensors=\"tf\")\n",
    "\n",
    "    # We convert the new encoded messages into a Dataset\n",
    "    new_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(new_encodings)\n",
    "    ))\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(new_dataset.batch(obj_model.batch_size))\n",
    "\n",
    "    # to convert the predictions in lable\n",
    "    max_indices = np.argmax(predictions['logits'], axis=1)\n",
    "    \n",
    "    for i, idx in enumerate(max_indices):\n",
    "        emotion = obj_clean.id11label[idx]\n",
    "\n",
    "        df.loc[i, emotion] = True\n",
    "\n",
    "\n",
    "    df = df.fillna(False)\n",
    "\n",
    "    return df#[obj_clean.id11label[index] for index in max_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c76bc7-1236-41c3-927a-5aaca3de7b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model('best_model_sentiment.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052cc27-b564-49f3-b67f-c3539396735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obj_clean_test.df['emotion'] = predict_(new_messages=obj_clean_test.df.text_clean.to_list(), model=model)\n",
    "obj_clean_test.df = predict_(obj_clean_test.df, new_messages=obj_clean_test.df.text_clean.to_list(), model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4d4dff-245b-4b90-a79b-6b0bc1a04a8f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "obj_clean_test.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5d7700-681d-44fe-9001-d73e2903e2b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj_clean_test.df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
